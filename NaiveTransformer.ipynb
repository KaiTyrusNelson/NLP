{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gHmDmA8hOJ_p",
    "outputId": "f5a6315f-c894-44db-eb6a-78d25afecd4e"
   },
   "source": [
    "# Transformers Notebook\n",
    "\n",
    "#### Summary\n",
    "\n",
    "This notebook showcases basic uses from transformers, as well as the from scratch implementation.\n",
    "\n",
    "In this notebook we will:\n",
    "* Write shakespeare via a Self Attentive Transformer Decoder\n",
    "* Translate English To French, via a Transformer Encoder and Decoder\n",
    "\n",
    "\n",
    "## Shakespeare\n",
    "\n",
    "### Data\n",
    "\n",
    "To begin, we will load our data from a txt file containing Shakespearean Plays\n",
    "\n",
    "*Data: https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GKXoHKgeOcSF"
   },
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "  text=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B3KP20GTOuv7",
    "outputId": "b0623d75-d643-45b9-fcad-005360a5ebc6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zESDirh8T3r1"
   },
   "source": [
    "#### TOKENIZER\n",
    "\n",
    "The tokenizer subdivides the sentences into tokens, which then have their semantic meaning learned by the model.\n",
    "\n",
    "* Started initially with a per character tokenizer\n",
    "* UPDATE: Replaced tokenizer with a pretrained BERT tokenizer (This is fine for English, however, later in the notebook is not as good for French)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qeleXgdjPEVq",
    "outputId": "d808d2fe-4902-45a2-d2f6-533fb3eb505e"
   },
   "outputs": [],
   "source": [
    "#NAIVE INTEGER TOKENIZER\n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "#Pretrained tokenizer\n",
    "enc = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "vocab_size = enc.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loader\n",
    "\n",
    "Now, we will create a basic method to load our data quickly for both train and validation. We will do this via a simple function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jj6zWyayPWDR",
    "outputId": "4cbfc98a-eff0-4bb9-a8e7-4a254df48b46"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.tensor(enc.encode(text))\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1iTaQY_yQPVy"
   },
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N2KCrVb_QV9q",
    "outputId": "4d8e82f7-3484-411a-b037-b46e98b98083"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1139,  1540,  1706,  ...,  2789,  1109,  1269],\n",
      "        [ 1189,  1103, 10136,  ...,  1699,   117,  3465],\n",
      "        [ 4199,   117,  6884,  ..., 23354,   131,   146],\n",
      "        ...,\n",
      "        [ 1262,  3041,  1240,  ...,  1103,  2089,  1104],\n",
      "        [  173,  2353,  5141,  ...,  2196,  1436,   119],\n",
      "        [ 1341,  4218,  1106,  ...,  1159,  1536,   119]])\n",
      "tensor([[ 1540,  1706,   184,  ...,  1109,  1269,   146],\n",
      "        [ 1103, 10136,  4455,  ...,   117,  3465,  1324],\n",
      "        [  117,  6884, 27296,  ...,   131,   146,  1821],\n",
      "        ...,\n",
      "        [ 3041,  1240,  6927,  ...,  2089,  1104,  1142],\n",
      "        [ 2353,  5141,   112,  ...,  1436,   119, 22870],\n",
      "        [ 4218,  1106,  9015,  ...,  1536,   119,  8784]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "batch_size = 128\n",
    "block_size = 32 # maximum context length\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data)-block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "print(xb)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n",
    "\n",
    "In order to predict the text, we will make an attention based transformer decoder. This will use self attention to take the current text, and predict the new text.\n",
    "\n",
    "NOTE: WE WILL USE LEARNED POSITIONAL EMBEDDINGS FOR SIMPLICITY; HOWEVER, SINUSOIDAL PE IS TYPICALLY BETTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9kpq0JhdEz6z",
    "outputId": "3edb483f-bb6c-4a95-fa32-a6f0854b0ebb"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "\n",
    "n_embed = 64\n",
    "\n",
    "head_size = 16\n",
    "\n",
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size, mask = True):\n",
    "        super().__init__()\n",
    "        self.key = torch.nn.Linear(n_embed, head_size, bias = False)\n",
    "        self.query = torch.nn.Linear(n_embed, head_size, bias = False)\n",
    "        self.value = torch.nn.Linear(n_embed, head_size, bias = False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.mask = mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        wei = q @k.transpose(-2, -1) / head_size**0.5\n",
    "        if self.mask:\n",
    "            wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = torch.softmax(wei, dim=-1)\n",
    "\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, head_size, mask = True):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, mask = mask) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(num_heads * head_size, n_embed)\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim =-1)\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, sz):\n",
    "        super().__init__()\n",
    "        self.FF = nn.Sequential(\n",
    "            nn.Linear(sz, sz*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(sz*4, sz),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.FF(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed, n_head, mask = True):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size, mask = mask)\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # THE EMBEDDING SPACE x->(z)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.positional_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(n_embed, n_head = 6),\n",
    "            Block(n_embed, n_head = 6),\n",
    "            Block(n_embed, n_head = 6),\n",
    "            nn.LayerNorm(n_embed)\n",
    "        )\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, S = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx) # B,T, C\n",
    "        pos_emb = self.positional_embedding_table(torch.arange(S)) # (T,C)\n",
    "\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            print(logits)\n",
    "            targets = targets.view(B*T)\n",
    "            print(targets)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            idx_cond = idx[:,-block_size:]\n",
    "            # GRABS THE NEXT PREDICTION\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:,-1,:]\n",
    "\n",
    "            # CREATES THE NEXT PIECE OF SEQUENCE\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1)\n",
    "\n",
    "            # ADDS CHOICE TO SEQUENCE\n",
    "            idx = torch.cat((idx, idx_next), dim = 1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "m = Decoder(vocab_size)\n",
    "#sequence = m.generate(xb, 100)\n",
    "#print(enc.decode(sequence[0,:].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BjMRsJ1fLKch"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4432, -0.0112,  0.3615,  ...,  0.2669, -0.6870,  0.9221],\n",
      "        [-0.6037,  0.1370,  0.5656,  ...,  0.7308,  0.3631,  0.2769],\n",
      "        [-0.1342,  0.1373,  0.0837,  ...,  0.4316, -0.9123,  0.0554],\n",
      "        ...,\n",
      "        [ 0.0745,  0.1808,  0.1682,  ..., -0.0261,  0.1757, -0.0440],\n",
      "        [-0.5510, -0.5618, -0.2221,  ...,  0.7620,  0.4578,  0.6521],\n",
      "        [ 0.4486,  0.0223, -0.9319,  ...,  0.4141, -0.5009,  1.4216]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([17604,  1131,  1439,  ...,   119,   148, 15740])\n",
      "10.424849510192871\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr = 3e-4)\n",
    "\n",
    "for steps in range(1000):\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    logits, loss = m(xb, yb)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "BjMRsJ1fLKch"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 32])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgWEHi8Xvwas"
   },
   "source": [
    "# English to French Translation\n",
    "\n",
    "We have assembled a basic self attention decoder, which is able to perform text prediction. We will now use the build blocks of this model to perform machine translation via an encoder decode model.\n",
    "\n",
    "*DataSet: https://www.kaggle.com/datasets/devicharith/language-translation-englishfrench*\n",
    "#### Cross Attention\n",
    "\n",
    "In order to perform translation, we need to implement what is called \"Cross Attention\". This will allow the decoder to query information from the features that the encoder generates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FLJ_omsTSIaU",
    "outputId": "afd94cba-b925-4742-b7ae-5a8c627f1efa"
   },
   "outputs": [],
   "source": [
    "class CrossAttentionHead(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = torch.nn.Linear(n_embed, head_size, bias = False)\n",
    "        self.query = torch.nn.Linear(n_embed, head_size, bias = False)\n",
    "        self.value = torch.nn.Linear(n_embed, head_size, bias = False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "    \n",
    "    \n",
    "        # PERFORMS CROSS ATTENTION\n",
    "    def forward(self, x, x_cross):\n",
    "        \n",
    "        B,T,C = x.shape\n",
    "\n",
    "        k = self.key(x_cross)\n",
    "        q = self.query(x)\n",
    "\n",
    "        wei = q @k.transpose(-2, -1) / head_size**0.5\n",
    "        #wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = torch.softmax(wei, dim=-1)\n",
    "\n",
    "        v = self.value(x_cross)\n",
    "        out = wei @ v\n",
    "\n",
    "        return out\n",
    "    \n",
    "class MultiHeadCrossAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([CrossAttentionHead(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(num_heads * head_size, n_embed)\n",
    "        \n",
    "    def forward(self, x, x_cross):\n",
    "        out = torch.cat([h(x = x, x_cross = x_cross) for h in self.heads], dim =-1)\n",
    "        out = self.proj(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class CrossAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.attn = MultiHeadAttention(n_head, head_size)\n",
    "        self.sa = MultiHeadCrossAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "        self.ln3 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x, x_cross):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.sa(x = self.ln2(x), x_cross = self.ln2(x_cross))\n",
    "        x = x + self.ffwd(self.ln3(x))\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer\n",
    "\n",
    "With the new cross attention modules, we will now be able to create our transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FLJ_omsTSIaU",
    "outputId": "afd94cba-b925-4742-b7ae-5a8c627f1efa"
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # THE EMBEDDING SPACE x->(z)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.positional_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        \n",
    "        # ENCODER\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(n_embed, n_head = 6, mask = False),\n",
    "            Block(n_embed, n_head = 6, mask = False),\n",
    "            Block(n_embed, n_head = 6, mask = False),\n",
    "            Block(n_embed, n_head = 6, mask = False),\n",
    "            Block(n_embed, n_head = 6, mask = False),\n",
    "            Block(n_embed, n_head = 6, mask = False),\n",
    "            nn.LayerNorm(n_embed)\n",
    "        )\n",
    "        \n",
    "        self.decoderBlocks = nn.ModuleList([CrossAttentionBlock(n_embed, n_head = 6) for i in range(6)])\n",
    "        \n",
    "        \n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x, x_cross, targets=None):\n",
    "        # TARGETS: WHAT WE WANT THE NEXT WORD OF THE PREDICTION TO BE\n",
    "        # X: THE CURRENT GENERATION THAT WE HAVE\n",
    "        # X_CROSS: THE TEXT WE ARE TRANSLATING\n",
    "        \n",
    "        B, S = x.shape\n",
    "\n",
    "        tok_emb_cross = self.token_embedding_table(x_cross) # B,T, C\n",
    "        tok_emb = self.token_embedding_table(x)\n",
    "        \n",
    "        #print(S)\n",
    "        #print(block_size)\n",
    "        #print(torch.arange(S))\n",
    "        \n",
    "        pos_emb = self.positional_embedding_table(torch.arange(S))\n",
    "\n",
    "        x = tok_emb + pos_emb\n",
    "        x_cross = tok_emb_cross + pos_emb\n",
    "\n",
    "        # RUN THE CROSS DATA THROUGH THE ENCODER\n",
    "        x_cross = self.blocks(x_cross)\n",
    "        \n",
    "        \n",
    "        for decoder in self.decoderBlocks:            \n",
    "            x = decoder(x = x, x_cross = x_cross)\n",
    "        \n",
    "        \n",
    "        # TURN THE RESULT INTO LOGITS\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            #print(logits)\n",
    "            targets = targets.view(B*T)\n",
    "            #print(targets)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx):\n",
    "        \n",
    "        gen = torch.zeros_like(idx)\n",
    "        \n",
    "        \n",
    "        for i in range(block_size):\n",
    "           \n",
    "            logits, loss = self.forward(gen ,idx)\n",
    "            logits = logits[:,i,:] \n",
    "            \n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1)\n",
    "            \n",
    "            #Set the vector slice to the next part\n",
    "            if (i < block_size-1):\n",
    "                gen[:,i+1:i+2] = idx_next\n",
    "            else:\n",
    "                return torch.concat([gen[:,1:], idx_next], axis = 1)\n",
    "        \n",
    "\n",
    "    \n",
    "m = Transformer(vocab_size)\n",
    "#m = torch.load('./Models/HighEmbedCurrent')\n",
    "#gen = m.generate(tar)[0,:].tolist()\n",
    "#gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Data Loader with Machine Translation\n",
    "\n",
    "Now that we have the transformer, we need to have a way to load the data. Our data loader create tensors for the English Sentence, the French Sentence, and the Prediction Targets for each point in the French Translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "keqlaGQfzBDC",
    "outputId": "ec119355-25da-48ab-b33a-9f6b01cfa3a3"
   },
   "outputs": [],
   "source": [
    "# NOW WE NEED TO LOAD THE DATA\n",
    "import pandas\n",
    "import numpy as np\n",
    "\n",
    "data = pandas.read_csv('fra.csv', sep = '\\t')\n",
    "data.columns = usecols = ['eng', 'fra']\n",
    "\n",
    "\n",
    "train_numbers = np.random.choice(range(len(data)), size= int(0.9*len(data)), replace=False)\n",
    "test_numbers = [i for i in range(len(data)) if i not in numbers]\n",
    "\n",
    "data['eng']=data['eng'].apply(enc.encode)\n",
    "data['fra']=data['fra'].apply(enc.encode)\n",
    "\n",
    "train = data.iloc[train_numbers]\n",
    "test = data.iloc[test_numbers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "470yw_cDzMGq"
   },
   "outputs": [],
   "source": [
    "\n",
    "# SOME OF OUR DATA IS LARGER THAN OUR BUFFER SIZE, FOR SIMPLICITY WE WILL JUST CUT THAT DATA OUT.\n",
    "\n",
    "def normalize_length(arr):\n",
    "    \n",
    "    arr = arr[:block_size] + [0] * ( max(0, block_size - len(arr)) ) \n",
    "    \n",
    "    return arr\n",
    "    \n",
    "    \n",
    "train['eng']=train['eng'].apply(normalize_length)\n",
    "train['fra']=train['fra'].apply(normalize_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train if split == 'train' else test\n",
    "    mx = len(data.index)\n",
    "    \n",
    "    ix = torch.randint(mx, (batch_size,))\n",
    "    \n",
    "    eng = torch.stack([torch.LongTensor(data.loc[int(i)]['eng']) for i in ix])\n",
    "    tar = torch.stack([torch.LongTensor(data.loc[int(i)]['fra']) for i in ix])\n",
    "    fra = torch.stack([torch.LongTensor([0] + data.loc[int(i)]['fra'][:block_size-1]) for i in ix])\n",
    "\n",
    "    return tar, fra, eng\n",
    "\n",
    "tar, fra, eng = get_batch('train')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "Now all there is left to do is train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr = 3e-4)\n",
    "\n",
    "for steps in range(5000):\n",
    "\n",
    "    tar, fra, eng = get_batch('train')\n",
    "    \n",
    "    logits, loss = m(x = fra, x_cross = eng, targets = tar)\n",
    "    \n",
    "    print(loss.item(), end = \"\\r\")\n",
    "    \n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (steps%1000 == 1):\n",
    "        torch.save(m,'./Models/HighEmbedEpoch'+str(steps) )\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "While not entirely accurate, the transformer is able to roughly translate some sentences. Currently, it is limited by training time, and complexity. Given a higher embed dimension, more attention heads, or a greater depth, along with ample training, it would be able to more accurately decode french sentences.\n",
    "\n",
    "Another major culprit is our tokenizer. The token are connected via MEANING (word embedding), the english tokenizer subdivides sentences into english tokens, thus resulting in token meaning not translating over well, as words are subdivided, with little meaning in embedding space.\n",
    "\n",
    "Finally, we used LEARNED POSITIONAL EMBEDDINGS. While these are sufficient for basic tasks, it is typically common-practice to use SINUSOIDAL EMBEDDINGS INSTEAD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar, fra, eng = get_batch('test')\n",
    "gen = m.generate(eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm in the mood for something sweet. Srsque j'avion les paies quelque chose.\n",
      "Is there any other way besides extraction? Y a - t - il d'autre une manque, a chamua plans?\n",
      "We got ready. Nous s'espérains présentée à nous passer.\n",
      "I liked your friends. J'appréciaère vos amis.\n",
      "She was educated by her grandfather. Elle fut la pi sciencee était par son grand -uand.\n",
      "I know neither of them. Je suis parti bord de le dénus.\n",
      "Thanks for supporting me. Merci d'avoir offens compées.\n",
      "I folded the towels. Je répépête surpris.\n",
      "I just want you to think about it. Je veux juste que tu y songes.\n",
      "Do you have children's clothes? Avez - vous des enfants?\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(enc.decode([n for n in eng[i,:].tolist() if n != 0]), enc.decode([n for n in gen[i,:].tolist() if n != 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
